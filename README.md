# ğŸ¥ Medical QA RAG System

An **AI-powered Medical Question Answering Chatbot** built using **Retrieval-Augmented Generation (RAG)**. The system retrieves relevant medical knowledge from documents and generates accurate, context-aware answers using a large language model.

Designed as a **production-style project**, this repository demonstrates practical experience with modern AI stacks including **LangChain, Pinecone, Groq LLMs, Flask and conversation memory buffer**.

---

## ğŸš€ Features

* ğŸ“„ **Document-based Medical QA** using RAG
* ğŸ” **Semantic search** with Pinecone Vector Database
* ğŸ¤– **LLM-powered answers** using Groq (LLaMA 3)
* ğŸ§  **HuggingFace embeddings** for document vectorization
* ğŸŒ **Flask web interface** for real-time chat
* ğŸ§  **Conversation memory buffer** to handle follow-ups, pronouns, and topic switching 
* ğŸ” Environment-based API key management

---

## ğŸ› ï¸ Tech Stack

| Category    | Tools            |
| ----------- | ---------------- |
| Language    | Python           |
| LLM         | Groq (LLaMA 3.1) |
| Framework   | LangChain        |
| Vector DB   | Pinecone         |
| Embeddings  | HuggingFace      |
| Backend     | Flask            |
| Frontend    | HTML, CSS        |
| Environment | Conda / venv     |

---

## ğŸ“‚ Project Structure

```
medical-QA-RAG-system/
â”‚
â”œâ”€â”€ app.py                 # Flask application entry point
â”œâ”€â”€ store_index.py         # Script to create & store embeddings in Pinecone
â”œâ”€â”€ requirements.txt       # Project dependencies
â”œâ”€â”€ setup.py               # Package configuration
â”œâ”€â”€ .env                   # Environment variables (ignored in git)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Medical_book.pdf   # Source medical document
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ helper.py          # Embedding & document loading logic
â”‚   â”œâ”€â”€ prompt.py          # System prompt for medical assistant
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ chat.html          # Chat UI
â”‚
â”œâ”€â”€ static/
â”‚   â””â”€â”€ style.css          # Styling
â”‚
â””â”€â”€ research/
    â””â”€â”€ trials.ipynb       # Experiments & testing
```

---

## ğŸ§© Architecture Overview

1. **Document Ingestion** â†’ PDF loaded & chunked
2. **Embeddings** â†’ HuggingFace embeddings generated
3. **Vector Store** â†’ Stored & searched via Pinecone
4. **Retrieval** â†’ Top-k relevant chunks fetched
5. **Conversation Memory Buffer** â†’ Tracks user conversation context, handles follow-ups, pronouns, and topic switches
6. **Generation** â†’ Groq LLaMA 3 produces grounded answers
7. **API/UI** â†’ Flask serves responses to a chat UI

---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/your-username/medical-QA-RAG-system.git
cd medical-QA-RAG-system
```

### 2ï¸âƒ£ Create & activate environment

```bash
conda create -n medibot python=3.10 -y
conda activate medibot
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configure environment variables

Create a `.env` file:

```env
PINECONE_API_KEY=your_pinecone_key
GROQ_API_KEY=your_groq_key
```

---

## ğŸ“¥ Index Medical Documents

Run once to store embeddings in Pinecone:

```bash
python store_index.py
```

---

## â–¶ï¸ Run the Application

```bash
python app.py
```

Open in browser:
ğŸ‘‰ `http://localhost:8080`

---

## ğŸ’¬ Example Workflow

1. User asks a medical question
2. Query is embedded using HuggingFace
3. Relevant medical chunks retrieved from Pinecone
4. Conversation memory buffer maintains context for follow-up questions
5. Context + query passed to Groq LLM
6. Accurate, grounded response returned to user

---

## âš ï¸ Medical Disclaimer

This chatbot is for **educational purposes only** and does **not** replace professional medical advice.

---

## ğŸ“Œ Resume Highlights

* Built an **end-to-end Medical RAG system** using LangChain
* Integrated **vector search (Pinecone)** with LLM inference
* Designed **Flask-based AI API & UI**
* Implemented **conversation memory buffer** for follow-ups and context
* Designed **scalable document ingestion pipeline**
* Followed **production best practices** (env vars, modular code)

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

## ğŸ‘¨â€ğŸ’» Author

**Rajnish Gupta**
Software Engineer | AI & Data Engineering Enthusiast

---

â­ If you found this project useful, consider giving it a star!
